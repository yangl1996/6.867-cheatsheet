\section{Probability}

$\mathbb{E}(AB) \geq \mathbb{E}(A)\mathbb{E}(B)$\\
$\text{cvxfunc}(E[X]) \leq E[\text{cvxfunc}(X)]$\\
Sigmoid function: $\sigma(x) = \frac{e^x}{1+e^x}$\\
$\sigma'(x) = \sigma(x)(1-\sigma(x))$\\
$\mathbb{P}(X, Y) = \mathbb{P}(X|Y)\mathbb{P}(Y) = \mathbb{P}(Y|X)\mathbb{P}(X)$\\
Cond. Independ.: $\mathbb{P}(X|Y, Z) = \mathbb{P}(X|Z)$\\
Chernoff Bound: Boolean $X$, $N$ i.i.d draws, $P(X_i=1)=p$, MLE of $p$ is $\bar{p}=\frac{1}{N}\sum x_i$. For all $p \in [0, 1]$, $\epsilon > 0$, $P(|p-\bar{p}| > \epsilon) \leq 2e^{-2N\epsilon^2}$\\
Hoeffding's inequality: $X \in [0, 1]$, $\mathbb{P}(\bar{X} - E[\bar{X}] \geq t) \leq e^{-2nt^2}$\\
Markov's inequality: for all $a > 0$ for nonnegative $Z$, $P(Z \geq a) \leq E[Z]/a$

\section{Matrix Differential}

$\frac{\partial Ax}{\partial z} = A\frac{\partial x}{\partial z}$\\
$\frac{\partial y^T A x}{\partial x} = y^T A$, $\frac{\partial y^T A x}{\partial y} = x^T A^T$\\
$\frac{\partial y^T x}{\partial z} = x^T\frac{\partial y}{\partial z} + y^T \frac{\partial x}{\partial z}$\\
$\frac{\partial y^T A x}{\partial z} = x^T A^T \frac{\partial y}{\partial z} + y^T A\frac{\partial x}{\partial z}$\\
$\frac{\partial A^{-1}}{\partial \alpha} = -A^{-1}\frac{\partial A}{\partial \alpha} A^{-1}$

\section{Foundations}

\subsection*{Probability Model \& Bayes Classifier}

$\eta(x) = \mathbb{P}(Y=1|X=x) = \mathbb{E}[Y|X=x]$\\
$L(h) = L_{\mathbb{P}}(h) = \mathbb{P}(h(X)\neq Y)$\\
$L^* = \inf_h \mathbb{P}(h(X) \neq Y) = \mathbb{E}[\min\{\eta(X), 1-\eta(X)\}] = \frac{1}{2} - \frac{1}{2}\mathbb{E}[|2\eta(X) - 1|]$

\subsection*{Nearest Neighbor Classifier}

$L_{NN} = \mathbb{E}[2\eta(X)(1-\eta(X))]$ asymptotically\\
$L^* \leq L_{NN} \leq 2L^*(1-L^*)$

\subsection*{Empirical Risk Minimization}

$\epsilon_{\text{apx}} = \min_{h \in H}L(h)$, $\epsilon_{\text{est}} = L_\mathbb{P}(h_S) - \epsilon_{\text{apx}}$\\
$\epsilon_{\text{est}}$ increases as $\log |H|$ decreases with $N$

\section{Linear classifiers}

\subsection*{Definitions}

$h(x) = \text{sgn}(f(x))$, $f(x) = w^T x + w_0$\\
Hinge loss: $l_{\text{h}}(z) = \max(0, 1-z)$\\
Logistic loss: $l_{\log}(z) = \log(1+e^{-z})$\\
0/1 loss: $l_{\text{0/1}} = [[z \leq 0]]$, $z = yf(x)$

\subsection*{Logistic Regression}

Model $\mathbb{P}(Y=1|X, w)$ as $\sigma(w x +w_0)$ and use this as $\eta(x)$ to build Bayes Classifier.\\
Max. likelihood $P(S|w) = \prod \mathbb{P}(y_i|x_i)$\\
Min. negative log-likelihood:\\
$L(w) = -\sum[y_i \log \sigma(w^T x_i) + (1 - y_i) \log(1 - \sigma(w^T x_i)]$ (cross entropy), or $\sum \log(1 + \exp(-y_i w^T x_i))$ when we use $y \in \{-1, 1\}$\\
Regularization: add $\lambda ||w||$ (convex)

\subsection*{Support Vector Machine}

$L_S = \frac{1}{2}||w||^2 + \frac{C}{N} \sum \max(0, 1-y_i(w^T x_i + w_0))$, $\min |w^T x_i + w_0| = 1$, margin $2/||w||$\\
Goal: maximize margin between SVs\\
$\mathbb{P}(\text{test err}) \leq \text{margin err} + \text{O}(1/\text{margin})$\\
Hard SVM: $\min ||w||^2/2$ given $y_i(w^T x_i + w_0) \geq 1$, solve using Lagrangians:\\
$L = \frac{1}{2} ||w||^2 - \sum \alpha_i [y_i(w^T x_i + w_0) - 1]$\\
Stationarity: ${\partial L}/{\partial w}=0$, ${\partial L}/{\partial w_0} = 0$\\
Complement: $\alpha_i [y_i (w^T x_i + w_0) - 1] = 0$\\
Primal Feasibility: $y_i (w^T x_i + w_0) \geq 1$\\
Dual Feasibility: $\alpha_i \geq 0$\\
$w = \sum \alpha_i y_i x_i$, $\sum \alpha_i y_i = 0$ (hyperplane is linear comb. of training data), $\alpha > 0$: SV\\
Soft SVM: $\min ||w||^2/2 + C \sum \epsilon_i$, given $y_i(w^T x_i + w_0) \geq 1 - \epsilon_i$, $\epsilon_i > 0$\\
$\epsilon$: violation distance (0 for margin met)\\
Dual: $\max (-1/2) ||\sum \alpha_i x_i y_i||^2 + \sum \alpha_i$, given $\sum y_i \alpha_i = 0$, $0 \leq \alpha_i \leq C$\\
$\alpha=0$: correct, $=C$: in margin, $< C$: SV\\
$\sum \alpha_i = \text{margin}^{-2}$\\
Kernel: $k(x, x') = \langle \phi(x), \phi(x') \rangle$, $w = \sum\alpha_i y_i \phi(x_i)$, $h(x) = \text{sgn}(\langle w, \phi(x) \rangle + w_0)$\\
Gaussian RBF: $\exp(-\lambda ||x - x'||^2)$\\
Laplacian RBF: $\exp(-\lambda ||x - x'||)$\\
Polynomial: $(\langle x, x' \rangle + c)^d$, $c \geq 0$

\section{Naive Bayes}

Generative: $h(x) = \text{argmax}_y \mathbb{P}(X=x, Y=y) = \mathbb{P}(X=x|Y=y)\mathbb{P}(Y=y)$\\
Discr.: $h(x) = \text{argmax}_y \mathbb{P}(Y=y|X=x)$\\
ERM: minimize $L_S(h)$ over $h \in H$\\
Discr. ignores data dist. $\mathbb{P}(X)$\\
Naive Bayes: assume cond. independ. of features given class, $\mathbb{P}(X|Y) = \prod \mathbb{P}(X_d|Y)$\\
$\text{argmax}_y \mathbb{P}(y|x) = \mathbb{P}(y) \prod \mathbb{P}(x_d|y)$\\
NB is linear classifier, $\epsilon(LR, \inf) \leq \epsilon(NB, \inf)$\\
For LR in $d$ dim., with high prob. $\epsilon(LR, N) \leq \epsilon(LR, \inf) + \text{O}(\sqrt{\frac{d}{N}\log\frac{N}{d}})$

\section{Learning Theory}

Realizability: there is a hypothesis $h^* \in H$ that $L(h^*) = 0$. This implies that over random sample $S$, the loss $L_S(h^*)=0$
(with prob$=1$) for items in $S$ drawn according to the data distribution. Realizability also implies that $L_S(h_S)=0$ for every ERM hypothesis $h_S$\\
PAC: Aim: Find $h$ in $H$ such that $L(h) \leq \epsilon$, can be attained. Assumption: $L(h^*)=0$ and thus $L_S(h_S)=0$ (i.e., 0 training error). Then for $N \geq N(\epsilon, \delta)$, $L(h_S) \leq \epsilon$, with prob $\geq 1-\delta$\\
Since training data $S$ is randomly drawn $\sim P(X,Y)$, there is a chance that it may be noninformative; hence a $\delta$ confidence parameter. Even when $S$ faithfully represents $P$, it may miss some details (it is just a finite sample) and cause our classifier to make minor errors; we tolerate these up to $\epsilon$\\
Any finite hypothesis class $H$ is PAC, and $N(\epsilon, \delta) \leq \text{O}(\log|H|\delta/\epsilon)$\\
PAC-learnability requires: estimation error should be bounded uniformly over all distributions for a given hyp. class. (apx. error depends on the fit of prior to the underlying distribution\\
Agnostic PAC: with high probability $L(h) \leq \inf_{h \in H} L(h) + \epsilon$ (come $\epsilon$-close to the best classifier in the hypothesis class)\\
$\epsilon$-representative sample: for all $h \in H$, $|L_S(h) - L_D(h)| \leq \epsilon$\\
If $\epsilon / 2$-representative, $L_D(h_S) \leq \min L_D(h) + \epsilon$\\
VC-dim: size of largest subset of $X$ that can be shattered by $H$ (every possible labelling, there is one $h \in H$ that can classify\\
Restriction: $H$ is class of functions from $X$ to $\{0, 1\}$, $C={c_1, c_2, \dots, c_m} \in X$. $H_C$ (restriction of $H$ to $C$) is the set of functions from $C$ to $\{0, 1\}$ that can be derived from $H$. $H_C = {(h(c_1), \dots, h(c_m)): h\in H}$, where each function is represented as a vector $\in \{0, 1\}^{|C|}$\\
Shattering: $|H_C| = 2^{|C|}$\\
The following are equivalent: $H$ has the uniform convergence property; Any ERM rule is a successful agnostic PAC learner for $H$; $H$ is agnostic PAC learnable; $H$ is PAC learnable; Any ERM rule is a successful PAC learner for $H$; H has a finite VC-dimension.\\
Growth function: $\tau_H(m)$ is the number of different function (labellings) from set $C$ of size $m$ to $\{0, 1\}$ that can be obtained by restricting $H$ to $C$.\\
Sauer: $\text{VCDim}(H) \leq d < \inf$, then $\tau_H(m) \leq \sum_{i=1}^d m\text{C}i$, and if $m > d + 1$ then $\tau_H(m) \leq (em/d)^d$.\\
For every distribution $P(X, Y)$ and every $\delta \in (0, 1)$, with prob. $1 - \delta$ over the choice of $S \sim P$, $|L_P(h) - L_S(h)| \leq \frac{4 + \sqrt{\log \tau_H(2N)}}{\delta \sqrt{2N}}$

\section{Linear Regression}

\subsection*{Set Up}

Correlation Coefficient: $r = \frac{1}{N} \sum \left(\frac{x_i-\bar{x}}{\sigma_x}\right)\left(\frac{y_i-\bar{y}}{\sigma_y}\right) = \frac{\text{cov}(x, y)}{\sigma_x \sigma_y}$\\
Least-squares Regression: $\min 1/N \sum(h(x_i) - y_i)^2$\\
Linear least-squares regression: $h(x) = w_0+ w^T x$\\
$w = \frac{\mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]}{\sigma_X^2} = r \cdot \frac{\sigma_Y}{\sigma_X}$\\
$\eta(x) = \mathbb{E}[Y|X = x]$ always performs the best\\

\subsection*{Linear least-squares}

$\min_w L(w) = \sum (y_i - w^T x_i)^2 = ||X w - y||^2$, $X \in \mathbb{R}^{N \times d}, y \in \mathbb{R}^N, w \in \mathbb{R}^d$\\
$L(w) = w^T X^T X w - 2 w^T X^T t + y^T y$\\
$\nabla L(w) = 2 X^T X w - 2 X^T y$\\
$w = (X^T X)^{-1} X^T y$\\
Observation Model: $y = w^T x + \eta$, $\eta \sim N(0, \sigma^2 I)$, observe true $x$ and noisy $y$, use them to get $\bar{w}$\\
Error: $1/N \sigma(\bar{y_i} - \mathbb{E}[y_i])^2 = 1/N ||X\bar{w} - Xw||^2$\\
As number of samples $N$ grows, error goes to 0

\subsection*{Kernelized Ridge Regression}

$y = w^T \phi(x) + w_0 = E[Y=y | X=x]$\\
$\min L(w) = 1/N \sum(y_i - w^T \phi(x_i))^2 + \lambda ||w||^2$\\
Solution: $w = \sum \alpha_i \phi(x_i)$\\
$w^T \phi(x) = \sum \alpha_i k(x_i, x)$\\
$\alpha = (K + N \lambda I)^{-1}y$\\

\subsection*{Regularization}

Observation: $y = f(x) + \eta$
Expected error: $\mathbb{E}[||y - \bar{y}||^2] =$ sum of inherent noise, squared bias, variance\\
Bias: $\mathbb{E}[\bar{f}(x) - f(x)]$\\
Variance: $\mathbb{E}[\bar{y}^2] - \mathbb{E}[\bar{y}]^2$\\
$\mathbb{E}[||y - \bar{f}||^2] = \mathbb{E}[||\eta||^2] + \mathbb{E}[||f - \mathbb{E}\bar{f}||^2] + \mathbb{E}[||\bar{f} - \mathbb{E}\bar{f}||^2]$\\
Gauss-Markov: Linear least-squares is the best (lowest variance) unbiased linear\\

\subsection*{Ridge Regression}

$\min L(w) = 1/N \sum(y_i - w^T x_i)^2 + \lambda ||w||^2$\\
$N \lambda w + X^T X w = X^T y$, so $w = (X^T X + N \lambda I)^{-1} X^T y$\\
$w_{\text{RR}} = (I + N \lambda (X^T X)^{-1})^{-1} w_{\text{LS}}$\\
$\text{Var}[w_{\text{RR}}] = \text{Var}[A w_{\text{LS}}] = A \text{Var}[w_{\text{LS}}] A^T$\\
$\text{Var}[w_{\text{LS}}] = \sigma^2 (X^T X)^{-1}$\\
Kernelized: define $Z = E[y]$, $\bar{z} = K(K+n\lambda I)^{-1}y$, bias: $\eta \lambda^2 z^T (K + n\lambda I)^{-2} z$, variance: $\frac{1}{n}\text{tr} C K^2 (K+n\lambda I)^{-2}$\\

\subsection*{LASSO Regression}

Can auto select relevant features (sparsity), small values are pushed to zero\\
Ridge: minimize $(y-w)^2 + \lambda w^2$, $w = y/(1+\lambda)$\\
LASSO: minimize $(y-w)^2 + \lambda |w|$, $w = y-\lambda/2$ for $y > \lambda/2$, $y + \lambda / 2$ for $y < -\lambda/2$, $0$ for otherwise\\

\subsection*{Bayesian View}

MLE: Model $P(Y|X)$ directly, assume $Y | X \sim N(w^T x, s)$\\
$w = \text{argmax} \prod P(y_i|x_i, w) = \text{argmax} log P = \text{argmax} - \sum (y_i - w^T x_i)^2$\\

Bayesian: priors $w \sim N(\mu_0, S_0)$, $P(w) = \exp (-1/2 (w - \mu_0)^T S_0^{-1} (w - \mu_0))$, $P(\text{data}|w) = \exp(-\frac{1}{2\sigma^2} ||y - X w||^2)$\\
Posterior $P(w|\text{data}) = P(\text{data}|w) \times P(w) = \exp(-\frac{1}{2\sigma^2} ||y - X w||^2 - \frac{1}{2}(w - \mu_0)^T S_0^{-1}(w-\mu_0)) = \exp(-\frac{1}{2} w^T J w + h^T w)$, where $J = S_0^{-1} + \sigma^{-2} X^T X$, $h = S_0^{-1}\mu_0 + \sigma^{-2} y^T X$\\
Standard form $N(\mu, \Sigma)$, $\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$\\
Information form $N^{-1}(h, J)$, $\exp(-\frac{1}{2}x^t J x + h^T x)$\\
$\Sigma^{-1} = J$, $\mu = J^{-1} h$\\
$P(w|\text{data})$ is Gaussian $N(\mu_N, S_N)$ with parameters $S_N^{-1} = J$, $\mu_N = S_N h$\\
$S_0 = s^2I$, $\mu_0 = 0$ leads to ridge regression\\
$y|w \sim N(w^T x, \sigma^2)$, $w \sim N(\mu_N, S_N)$\\
$E[y] = E[w]^T x = \mu_N^T x$, $\text{Var}[y] = \sigma_N^2(x) = x^T S_N x + \sigma^2$

\section{Neural Networks}

NN can approximate any continuous function arbitrarily well if and only if the activation function is not a polynomial\\
Rectifier: $\max(0, z)$\\


\subsection*{Training}
$\min R_N(\theta) = \frac{1}{N}\sum l(y_i, F(x_i, \theta))$\\
$l(y, z) = max(0, 1-yz)$, $l(y, z) = \frac{1}{2}(y-z)^2$\\
SGD: $\theta \leftarrow \theta - \eta \frac{\partial l(y, F(x; \theta))}{\partial \theta}$\\
$a^l = (a_1^l, a_2^l, \dots, a_{m'}^l)$, $m'$ units in layer $l$\\
$z^l = (W^l)^T a^{l-1} + b^l$\\
$a^l = f(z^l)$\\
$\frac{\partial l}{\partial W^l} = \frac{\partial z^l}{\partial W^l}[\frac{\partial l}{\partial z^l}] = \frac{\partial z^l}{\partial W^l}(\delta^l)^T$\\
$\delta^l = \frac{\partial l}{\partial z^l} = \frac{\partial z^{l+1}}{\partial z^l}\cdot\frac{\partial l}{\partial z^{l+1}} = \frac{\partial z^{l+1}}{\partial z^l} \cdot \delta^{l+1}$\\
$z^{l+1} = (W^{l+1})f(z^l) + b^{l+1}$, so $\frac{\partial z^{l+1}}{\partial z^l} = \text{Diag}[f'(z^l)]W^{l+1}$\\
$\delta^l = \text{Diag}[f'(z^l)]W^{l+1}\cdot\text{Diag}[f'(z^{l+1})]W^{l+2} \cdot \dots \cdot W^{L}\delta^L$\\
$\delta^L = \text{Diag}[f'(z^L)]\frac{\partial l}{\partial a^L}$\\
Gradients used in Backprop: $\frac{\partial l}{\partial W^l} = a^{l-1}\delta^l$, $\frac{\partial l}{\partial b^l} = \delta^l$\\
Dropout: turn off units when training, cut weights when testing\\
RNN: $s_t = \tanh(W^{s,s}s_{t-1} + W^{s, x}x_t)$\\
