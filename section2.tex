\section{When Train $\neq$ Test}

Test distribution $q(x, y)$ differs from training $p(x, y)$.\\
Transfer learning: labeled $p(x, y)$, labeled $q(x, y)$.\\
Importance reweighting: labeled $p$, unlabeled $q$.\\
Robustness to perturbations: labeled $p$, no $q$.

\subsection*{Transfer Learning}

Transfer learning for linear model: learn $w_{old}$ using $p$, then learn from $q$ using L1 or L2 regularization $||w-w_{old}||$. This corresponds to MAP estimation with a different prior.\\
Transfer learning for NN: cut top levels, train a small model to classify from existing features.\\
Transfer learning for RNN: use pre-trained word embeddings.

\subsection*{Importance Reweighting}

Objective: $\min_\theta E_{x, y\sim q}[L(x, y; \theta)]$ using samples from $p(x, y)$.\\
$\frac{1}{|S|}\sum_{t=1}^{|S|}\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)$ is an unbaised estimator of $E_{x, y\sim q}[L(x, y; \theta)]$, where $S=\{(x^t, y^t)\sim p\}$.\\
Proof: $E_S[\frac{1}{|S|}\sum_{t=1}^{|S|}\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)]=E_p[\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)]=\int_{x, y}p(x,y)\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)=\int_{x, y}q(x, y)L(x, y; \theta) = E_q[L(x, y; \theta)]$.\\
$\beta = q(x, y) / p(x, y)$.\\
Variance could be high when $\beta$ is large, may penalize $||\beta||$.\\
SVM: $\min_w \frac{1}{2} ||w||^2 + C \sum_t \beta_t \max(0, y_t(1-wx_t))$.\\
LR: $\min_w \frac{\lambda}{2}||w||^2 + \sum_t -\beta_t \log p(y_t|x_t; w)$.\\
Ridge: $w=(\lambda \text{diag}(\beta^{-1})+ X^TX)^{-1}X^TY$.\\
Class Imbalance: $\beta = \frac{q(x, y)}{p(x, y)} = \frac{q(y)q(x|y)}{p(y)p(x|y)} = \frac{q(y)}{p(y)}$. Notice $p(x|y)=q(x|y)$.\\
Selection Bias: For sample $(x, y) \sim q$, flip coin $S \sim p(S | x; \lambda)$. If $S=1$ reject, $S=0$ keep the sample. $p(x, y) = \frac{q(x, y) p(S=1|x; \lambda)}{\int_{x, y}q(x, y) p(S=1|x; \lambda)} = Cq(x, y)p(S=1|x; \lambda)$ (marginalize over $S$).\\
Covariate Shift: $p(x)$ changes but $p(y|x)$ does not. $q(x, y) = q(x)p(y|x)$, $\beta = q(x)/p(x)$. Only need unlabeled data from $q$ to correct.

\subsection*{Adversarial Robustness}

Objective: An adversary who know $w$ (for linear model) and $x$, $y$ (input and true label) can choose $k$ features to delete fro $x$ to screw up prediction.\\
$\min_w \max_{\alpha_1, \dots, \alpha_n} \frac{1}{2}||w||^2+C\sum_i h^{wc}(w, y_i x_i)$.\\
$h^{wc}(w, y_i x_i) = max(0, 1-y_iw(x_i \odot (1-\alpha_i))$ (worst-case hinge loss), $\alpha_i\in\{0, 1\}^d$ selects features to delete, $\sum_j \alpha_{ij} = k$.\\
Solution: Apply adversarial deletion in SGD at each step.

\section{PCA}

Find structure and learn relevant subspaces.\\
Data: $x_1, \dots, x_N \in \mathbb{R}^d$, project into $\mathbb{R}^m$, $m << d$.

\subsection*{Max. Variance}

First consider projecting to $1$-dimension: $x_n \rightarrow u_1^T x_n$, $n \in [N]$, $u_1 \in \mathbb{R}^d$, $u_1^T u_1 = 1$.\\
Mean: $u_1^T\bar{x}$.\\
Variance is $\frac{1}{N}\sum_n(u_1^T x_n - u_1^T \bar{x})^2 = u_1^T S u_1$, $S = \frac{1}{N}\sum_n (x_n - \bar{x})(x_n - \bar{x})^T$.\\
Goal: Max. $u_1^T S u_1$ s.t. $u_1^T u_1 = 1$.\\
Lagrangian: $L(u_1, \lambda_1) = u_1^T S u_1 - \lambda_1(u_1^T u_1 - 1)$, $\nabla_{u_1}L = 2 S u_1 - 2 \lambda_1 u_1$.\\
Sol.: $S u_1 = \lambda_1 u_1$, $u_1^T S u_1 = \lambda_1$. Meaning that $u_1$ is eigenvector, $\lambda_1$ is eigenvalue of $S$.\\
Project to $m$-dimension, $x_n \rightarrow (u_1^T x_n, \dots, u_m^T x_n)$, $n \in [N]$. $u_1, \dots, u_m$ are top $m$ eigenvectors of $S$.

\subsection*{Min. Projection Error}

$u_1, \dots, u_d$ are orthogonal unit basis of $\mathbb{R}^d$.\\
Affine projection on $\mathbb{R}_m$ ($m < d$): $x_n \rightarrow \tilde{x}_n = \sum_{i=1}^m z_{in}u_i + \sum_{i=m+1}^d b_i u_i$.\\
Goal: Min. $J = \frac{1}{N} \sum_n (x_n - \tilde{x}_n)^T(x_n - \tilde{x}_n)$.\\
Let $x_n = \sum_{i=1}^d \alpha_{in}u_i$, $\alpha_{in} = u_i^T x_n$ (since $u$ is basis). $J = \frac{1}{N}(\sum_{i=1}^m \sum_n (\alpha_{in} - z_{in})^2) + \frac{1}{N}(\sum_{i=m+1}^d \sum_n (\alpha_{in} - b_i)^2)$.\\
Sol.: $z_{in} = \alpha_{in}$ ($i \leq m$). $b_i = \frac{1}{N}\sum_n \alpha_{in} = u_i^T \bar{x}$ ($m < i \leq d$).\\
$J = \sum_{i=m+1}^d(u_i^T \bar{x} - u_i^T x_n)^2 = \sum_{i=m+1}^d u_i^T S u_i$. Pick $u_1, \dots, u_m$ to be top eigenvectors of $S$.

\subsection*{PCA Applications}

Encoder, decoder: $U = [u_1, \dots, u_m] \in \mathbb{R}^{d \times m}$. $x \rightarrow U^T x \rightarrow UU^T x$, first step is encoding, second step is decoding.\\
Principle Component Regression: eigendecomposition ($X^T X = U \Lambda U^T$, $X$ is centered data matrix), encode ($x_n \rightarrow U^T x_n = \tilde{x}_n$), Ordinary Least Sq. ($\hat{\beta}_m = (\tilde{X}^T\tilde{X})^{-1}\tilde{X}^Ty$), decode ($\hat{\beta} = U \hat{\beta}_m$).

\subsection*{PCA Implementation}

Data matrix: $X = [x_1 - \bar{x}, \dots, x_N - \bar{x}]^T \in \mathbb{R}^{N \times d}$.\\
Covariance matrix: $S = \frac{1}{N}X^TX$. $\frac{1}{N}X^TXu_i = \lambda_i u_i$.\\
$\frac{1}{N}XX^T(Xu_i) = \lambda_i (Xu_i)$, so $\frac{1}{N}XX^T v_i = \lambda_i v_i$ where $v_i = X u_i$. So $v_i$ is eigenvector of $Q = \frac{1}{N}XX^T$, $u_i \propto X^T v_i$.\\
$Xu_i \propto XX^T v_i \propto Qv_i \propto \lambda_i v_i$.

\subsection*{Non-linear PCA}

Centered data ($\bar{x} = 0$), non-linear $x \rightarrow \phi(x)$. $S' = \frac{1}{N}\sum_n \phi(x_n) \phi(x_n)^T$, top eigenvectors $S'v_i = \lambda_i v_i$, $i \in [m]$.\\
$\frac{1}{N}\sum_n \phi(x_n)(\phi(x_n)^T v_i) = \lambda_i v_i$, so $v_i = \sum_n a_{in}\phi(x_n)$.\\
$S' v_i = \frac{1}{N}\sum_n \phi(x_n)\phi(x_n)^T \sum_p a_{ip}\phi(x_p) = \lambda_i \sum_p a_{ip}\phi(x_p)$, $\frac{1}{N}\sum_n \phi(x_l)^T \phi(x_n) \sum_p a_{ip}\phi(x_n)^T \phi(x_p) = \lambda_i \sum_p a_{ip}\phi(x_l)^T \phi(x_p)$, $\frac{1}{N}\sum_n K(x_l, x_n) \sum_p a_{ip}K(x_n, x_p) = \lambda_i \sum_p a_{ip}K(x_l, x_p)$.\\
$K^2 a_i = \lambda_i NKa_i$, $Ka_i = \lambda_i Na_i$.\\
Projection: ($i$-th dim of projected $x$) $y_i(x) = \phi(x)^T v_i = \sum_n a_{in}\phi(x)^T\phi(x_n) = \sum_n a_{in} K(x, x_n)$.

\subsection*{Spiked Covariance Model}

Latent direction of interest: $v \in \mathbb{R}^d$, $v^Tv = ||v||^2 = 1$.\\
Data: $X_i = Y_i v + Z_i$, $Y_i \sim N(0, 1)$, $Z_i \sim N(0, \sigma^2I_{d\times d})$.\\
Mean: $E[X_i] = E[Y_i]v + E[Z_i] = 0$.\\
Covariance: $\Sigma = E[X_iX_i^T] = E[Y_i^2]vv^T + E[Z_iZ_i^T] = vv^T + \sigma^2I$.\\
Principle component (eigenvector) of covariance: $(vv^T + \sigma^2 I)v=v(v^Tv)+\sigma^2 v = (1+\sigma^2)v$.\\
Latent direction of interest captured by eigenvector of covariance matrix. PCA does that using empirical covariance.\\
Empirical Cov.: $S = \frac{1}{N}\sum_i X_i X_i^T$.\\
Use matrix concentration: with prob. $1-\delta$, $||S-\Sigma||_{\text{op}} \leq \text{const} ||\Sigma||_{\text{op}} \min(\sqrt{\frac{d+\log\frac{1}{\delta}}{N}}, \frac{d+\log\frac{1}{\delta}}{N})$. i.e., when $N \sim d$, $S \approx \Sigma$. If $k$-sparse, replace $d$ with $k$.\\
Use Davis-Kahan $\sin(\theta)$: $A$, $B$ are $d\times d$ PSD. $(\lambda_i, u_i)$, $(\mu_i, v_i)$ ranked eigenvalue/vector-pairs. $\sin(\angle(u_1, v_1))\leq \frac{2}{\max(\lambda_1 - \lambda_2, \mu_1 - \mu_2)} ||A-B||_{\text{op}}$. And $\min_{\epsilon\in{\pm1}}|\epsilon u_1 - v_1|_2^2 \leq 2 \sin^2(\angle(u_1, v_1))$.

\subsection*{Random Projection}

Data: $x_1, \dots, x_N$, $x_n \in \mathbb{R}^d$. Project to $\mathbb{R}^m$ and preserve distances.\\
Method: sample $m$ random vectors $W_1, \dots, W_m \in N(0, I_{d\times d})$, project $x_n$ as $(W_1^Tx_n, \dots, W_m^Tx_n)$.\\
Johnson-Lindenstrauss: $\epsilon, \delta \in (0, 1)$, let $m \geq \text{const}\frac{\log N/\delta}{\epsilon^2}$. Then for $1 \leq i, j \leq N$, $1-\epsilon \leq \frac{\frac{1}{\sqrt{m}}||W x_i - W x_j||_2}{||x_i - x_j||_2} \leq 1+\epsilon$ with prob. $1-\delta$. (i.e. projecting into $\log N$-dim preserves distance.)\\
Proof: Let $x = x_i - x_j$ for $||x||_2=1$ then we interested in $\frac{1}{\sqrt{m}}||Wx||_2$. Since $Wx \sim N(0, I_{m\times m})$, $||Wx||_2^2 = \sum_{i=1}^m Z_i^2$ where $Z_i \sim N(0, 1)$. Use Chernoff, $P(|\frac{1}{m}\sum_{i=1}^m Z_i^2-1| \geq \epsilon) \leq 2 \exp(-m\epsilon^2/6)$. Select $m \geq \text{const}\frac{\log N / \delta}{\epsilon^2}$.

\section{Matrix Estimation}

\subsection*{Model}
Ground truth: $A = [A_{ij}] \in \mathbb{R}^{n \times m}$.\\
Noisy observations: $Y_{ij}$ for $(i, j) \in \Omega \subseteq [n] \times [m]$. $E[Y_{ij}] = A_{ij}$.\\
Goal: Produce estimate $\hat{A}_{ij}$, $\forall (i, j) \in [n] \times [m]$ that $\hat{A}_{ij}\approx A_{ij}$ (MSE).\\
Model: sample latent feature $x_1(i) \in \chi_1$ for rows $i \in [n]$, $x_2(j) \in \chi_2$ for cols $j \in [m]$. Ground truth $A$ has $A_{ij} = f(x_1(i), x_2(j))$ ($f$ is latent function). Observation $Y_{ij}$ has $E[Y_{ij} | x_1(i), x_2(j)] = A_{ij}.$ Observe each $Y_{ij}$ with prob. $p$.

\subsection*{Singular Value Thresholding}

Replace missing $Y_{ij}$ with $0$.\\
Compute SVD of $Y$: $Y = \sum_{k=1}^{\min(m, n)}\sigma_k u_k v_k^T$. $u_k \in \mathbb{S}_n$, $v_k \in \mathbb{S}_m$, $\sigma_k \geq 0$, $\mathbb{S}$ is unit sphere.\\
Pick top components and rescale: $\hat{A} = \alpha \sum_{k \in S} \sigma_k u_k v_k^T$, $S = \{k: \sigma_k \geq \text{threshold}\}$, $\alpha=mn/|\Omega|$.

\subsection*{Convex Optimization}

Minimize $||Z||_*$ over $R^{n\times m}$ s.t. $Z_{ij} \approx Y_{ij}$ for $(i, j) \in \Omega$.\\

\subsection*{Non-convex Optimization}

Minimize MSE. Let $Z$ have rank $r$, $Z = UV^T$, $U \in \mathbb{R}^{n \times r}$, $V \in \mathbb{R}^{m \times r}$. Minimize $\sum_{(i, j)\in\Omega}(U_i^T V_j - Y_{ij})^2$ over $U_i, V_j \in \mathbb{R}^r$.\\
Alternative Least Squares: Iteratively fix $U$ and $V$ and optimize the other (solves Ordinary Least Sqaures).\\
Gradient Descent: $F = \sum_{(i, j)\in \Omega}(\sum_{q=1}^r U_{iq} V_{jq} - Y_{ij})^2$, $\frac{\partial F}{\partial U_{il}} = 2 \sum_{j \in \Omega} (\sum_q U_{iq}V_{jq} - Y_{ij})V_{jl}$, $\frac{\partial F}{\partial V_{jl}} = 2 \sum_{i \in \Omega} (\sum_q U_{iq}V_{jq} - Y_{ij})U_{il}$.

\subsection*{Collaborative Filtering}

Find most similar user/item and use that rating. Can do weighted.

\section{Clustering}

\subsection*{K-Means}

Pick $K$ random points as cluster centers. Assign points to nearest center. Move center to average of assigned points. Stop when no assignment changes.\\
Theorem: each iter. monotonically decreases the objective $\sum_{i=1}^N \min_{k\in1:K}||x_i - \mu_k||_2^2$, $\mu_k$ is cluster center.

\subsection*{Agglomerative Clustering}

Maintain a set of clusters. Initially each instance in its own cluster. Repeat: pick two closest clusters; merge them; stop when only one cluster left.\\
Produces a family of clusterings like a tree diagram.\\
Def. of closest: closest pair (single-link), farthest pair (complete-link), avg. of all pairs.\\
Strict ordering: all points are more similar to points in own cluster than any point in other cluster.\\
Theorem: if ground truth clustering has strict ordering, it can be found by single-link clustering.

\subsection*{Gaussian Mixture Models}

$P(Y)$: choose a cluster $Y=1, \dots, K$ ($K$ clusters).\\
$P(X|Y)$: each cluster generates data $\sim N(\mu_k, \Sigma_k)$.\\
$p(x) = \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)$, $\pi_k=P(y=k)$ (mixing component).\\
Learn with maximizing marginal likelihood of $x$: $\max_{\{\pi\}, \{\mu\}, \{\Sigma\}}\sum_{i=1}^N\log p(x_i; \{\pi\},\{\mu\},\{\Sigma\})$.\\
Expectation maximization: $\pi, \mu, \Sigma \leftarrow \text{argmax}_{\pi, \mu, \Sigma}\sum_{i=1}^N E_{p(y|x_i;\pi,\mu,\Sigma)}[\log p(y,x_i;\pi,\mu,\Sigma)]$.\\
Two-cluster EM: responsibility $\gamma_i = P(y=2|x_i;\pi,\mu,\sigma^2) =  \frac{\pi\phi_2(x_i)}{(1-\pi)\phi_1(x_i)+\pi\phi_2(x_i)}$, $i=1, \dots, N$, $\phi$ is CDF of normal distribution. $\mu_1=\frac{\sum(1-\gamma_i)x_i}{\sum(1-\gamma_i)}$, $\sigma_1^2=\frac{\sum(1-\gamma_i)(x_i-\mu_i)^2}{\sum(1-\gamma_i)}$. For $\mu_2$, $\sigma_2^2$ replace $(1-\gamma_i)$ with $\gamma_i$. $\pi = \sum \gamma_i/N$

\section{Topic Models}

\subsection*{Bayesian Network}

Specified as a directed acyclic graph $G = (V, E)$. One node $i \in V$ for each random variable $X_i$, one conditional prob. distribution per node $p(x_i|x_{\text{pa}(i)})$ ($\text{pa}$ for parent) specifying the variable's prob. conditioned on parents. Shaded node is evidence.\\
$p(x_1, \dots, x_n) = \prod_{i\in V}p(x_i|x_{\text{pa}(i)})$.\\
Plate notation: variables within a place replicated, conditionally independent.

\subsection*{Mixture Model}

$\theta \rightarrow z_d \rightarrow w_{id} \leftarrow \beta$.\\
Topic-word distributions: $\beta$.\\
Prior dist. over topics: $\theta$.\\
Topic of doc $d$: $z_d$ ($d=1:D$).\\
Word: $w_{id}$ ($i = 1 : N$).\\
Sample topic $z|\theta \sim \theta$ (single topic each document). Sample word $w_i|z \sim \beta_z$. $\{\beta_t\}_{t=1}^T$ are topics, $\beta_t=p(w|z=t)$.

\subsection*{Admixture Model (LDA)}

$\alpha \rightarrow \theta_d \rightarrow z_{id} \rightarrow w_{id} \leftarrow \beta$.\\
Dirichlet Hyperparams: $\alpha$.\\
Topic dist. for doc: $\theta_d$.\\
Topic of word $i$ of doc $d$: $z_{id}$.\\
Word: $w_{id}$.\\
Sample topic dist. (topic vector) $\theta \sim \text{Dirichlet}(\alpha_{1:T})$. For each word $i$ sample topic $z_i | \theta \sim \theta$. Then sample actual word $w_i | z_i \sim \beta_{z_i}$.\\
Inference: compute $p(\theta|w)$ i.e. $\forall t, E[\theta_t | w] = \int \theta_t p(\theta|w) d\theta$. Requires marginalizing over all word-topic assignments $p(\theta|w) = \sum p(\theta,z|w) = \sum p(z|w)p(\theta|z)$.\\

\subsection*{Variational Inference}

Construct optimization problem over dist. $q(\theta)$ s.t. only need to compute $p(\theta, w)$ and $q(\theta) \approx p(\theta|w)$.\\
$p(\theta, w) = \sum_z p(\theta,z,w) = \sum_z p(\theta) p(z|\theta) p(w|z) = \sum_z p(\theta) \prod_{i=1}^N p(z_i|\theta) p(w_i|z_i) = p(\theta) \prod_{i=1}^N \sum_{z_i} p(z_i|\theta) p(w_i|z_i)$.\\
KL-divergence: $D(p||q) = \sum_z p(z)\log\frac{p(z)}{q(z)}$. (asymmetric)\\
M-projection: $\text{argmin}_q D(p||q)$. I-projection: $\text{argmin}_q D(q||p)$. Differ when $q$ minimized over restricted set $Q$ and $p\not\in Q$.\\
Latent model: $p(z, x)$, observe $X=x$, calc $p(z|x) \approx \text{argmin}_{q(z) \in Q} D(q(z)||p(z|x))$.\\
$D(q||p) = \sum_z q(z)\ln \frac{q(z)}{p(z|x)} = \sum_z q(z)\ln\frac{q(z)p(x)}{p(z,x)} = \sum_z q(z)\ln\frac{q(z)}{p(z,x)}+\sum_z q(z)\ln p(x) = \sum_z q(z)\ln\frac{q(z)}{p(z,x)}+\ln p(x)$. Only need to min. $\sum_z q(z)\ln\frac{q(z)}{p(z,x)}$.\\
Naive mean-field: $q(z) = \prod_{i=1}^m q(z_i)$.\\
Mean-field: $q(\theta, z; \gamma, \phi) = q(\theta; \gamma) \prod_{i=1}^N q(z_i; \phi_i)$. $q(\theta; \gamma)$: Dirichlet with parameter $\gamma$; $q(z_i | \phi_i)$: multinomial with parameter $phi_i$.\\

