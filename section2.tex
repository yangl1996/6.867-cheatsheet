\section{When Train $\neq$ Test}

Test distribution $q(x, y)$ differs from training $p(x, y)$.\\
Transfer learning: labeled $p(x, y)$, labeled $q(x, y)$.\\
Importance reweighting: labeled $p$, unlabeled $q$.\\
Robustness to perturbations: labeled $p$, no $q$.

\subsection*{Transfer Learning}

Transfer learning for linear model: learn $w_{old}$ using $p$, then learn from $q$ using L1 or L2 regularization $||w-w_{old}||$. This corresponds to MAP estimation with a different prior.\\
Transfer learning for NN: cut top levels, train a small model to classify from existing features.\\
Transfer learning for RNN: use pre-trained word embeddings.

\subsection*{Importance Reweighting}

Objective: $\min_\theta E_{x, y\sim q}[L(x, y; \theta)]$ using samples from $p(x, y)$.\\
$\frac{1}{|S|}\sum_{t=1}^{|S|}\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)$ is an unbaised estimator of $E_{x, y\sim q}[L(x, y; \theta)]$, where $S=\{(x^t, y^t)\sim p\}$.\\
Proof: $E_S[\frac{1}{|S|}\sum_{t=1}^{|S|}\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)]=E_p[\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)]=\int_{x, y}p(x,y)\frac{q(x^t, y^t)}{p(x^t, y^t)}L(x^t, y^t; \theta)=\int_{x, y}q(x, y)L(x, y; \theta) = E_q[L(x, y; \theta)]$.\\
$\beta = q(x, y) / p(x, y)$.\\
Variance could be high when $\beta$ is large, may penalize $||\beta||$.\\
SVM: $\min_w \frac{1}{2} ||w||^2 + C \sum_t \beta_t \max(0, y_t(1-wx_t))$.\\
LR: $\min_w \frac{\lambda}{2}||w||^2 + \sum_t -\beta_t \log p(y_t|x_t; w)$.\\
Ridge: $w=(\lambda \text{diag}(\beta^{-1})+ X^TX)^{-1}X^TY$.\\
Class Imbalance: $\beta = \frac{q(x, y)}{p(x, y)} = \frac{q(y)q(x|y)}{p(y)p(x|y)} = \frac{q(y)}{p(y)}$. Notice $p(x|y)=q(x|y)$.\\
Selection Bias: For sample $(x, y) \sim q$, flip coin $S \sim p(S | x; \lambda)$. If $S=1$ reject, $S=0$ keep the sample. $p(x, y) = \frac{q(x, y) p(S=1|x; \lambda)}{\int_{x, y}q(x, y) p(S=1|x; \lambda)} = Cq(x, y)p(S=1|x; \lambda)$ (marginalize over $S$).\\
Covariate Shift: $p(x)$ changes but $p(y|x)$ does not. $q(x, y) = q(x)p(y|x)$, $\beta = q(x)/p(x)$. Only need unlabeled data from $q$ to correct.

\subsection*{Adversarial Robustness}

Objective: An adversary who know $w$ (for linear model) and $x$, $y$ (input and true label) can choose $k$ features to delete fro $x$ to screw up prediction.\\
$\min_w \max_{\alpha_1, \dots, \alpha_n} \frac{1}{2}||w||^2+C\sum_i h^{wc}(w, y_i x_i)$.\\
$h^{wc}(w, y_i x_i) = max(0, 1-y_iw(x_i \odot (1-\alpha_i))$ (worst-case hinge loss), $\alpha_i\in\{0, 1\}^d$ selects features to delete, $\sum_j \alpha_{ij} = k$.\\
Solution: Apply adversarial deletion in SGD at each step.

\section{PCA}

Find structure and learn relevant subspaces.\\
Data: $x_1, \dots, x_N \in \mathbb{R}^d$, project into $\mathbb{R}^m$, $m << d$.

\subsection*{Max. Variance}

First consider projecting to $1$-dimension: $x_n \rightarrow u_1^T x_n$, $n \in [N]$, $u_1 \in \mathbb{R}^d$, $u_1^T u_1 = 1$.\\
Mean: $u_1^T\bar{x}$.\\
Variance is $\frac{1}{N}\sum_n(u_1^T x_n - u_1^T \bar{x})^2 = u_1^T S u_1$, $S = \frac{1}{N}\sum_n (x_n - \bar{x})(x_n - \bar{x})^T$.\\
Goal: Max. $u_1^T S u_1$ s.t. $u_1^T u_1 = 1$.\\
Lagrangian: $L(u_1, \lambda_1) = u_1^T S u_1 - \lambda_1(u_1^T u_1 - 1)$, $\nabla_{u_1}L = 2 S u_1 - 2 \lambda_1 u_1$.\\
Sol.: $S u_1 = \lambda_1 u_1$, $u_1^T S u_1 = \lambda_1$. Meaning that $u_1$ is eigenvector, $\lambda_1$ is eigenvalue of $S$.\\
Project to $m$-dimension, $x_n \rightarrow (u_1^T x_n, \dots, u_m^T x_n)$, $n \in [N]$. $u_1, \dots, u_m$ are top $m$ eigenvectors of $S$.

\subsection*{Min. Projection Error}

$u_1, \dots, u_d$ are orthogonal unit basis of $\mathbb{R}^d$.\\
Affine projection on $\mathbb{R}_m$ ($m < d$): $x_n \rightarrow \tilde{x}_n = \sum_{i=1}^m z_{in}u_i + \sum_{i=m+1}^d b_i u_i$.\\
Goal: Min. $J = \frac{1}{N} \sum_n (x_n - \tilde{x}_n)^T(x_n - \tilde{x}_n)$.\\
Let $x_n = \sum_{i=1}^d \alpha_{in}u_i$, $\alpha_{in} = u_i^T x_n$ (since $u$ is basis). $J = \frac{1}{N}(\sum_{i=1}^m \sum_n (\alpha_{in} - z_{in})^2) + \frac{1}{N}(\sum_{i=m+1}^d \sum_n (\alpha_{in} - b_i)^2)$.\\
Sol.: $z_{in} = \alpha_{in}$ ($i \leq m$). $b_i = \frac{1}{N}\sum_n \alpha_{in} = u_i^T \bar{x}$ ($m < i \leq d$).\\
$J = \sum_{i=m+1}^d(u_i^T \bar{x} - u_i^T x_n)^2 = \sum_{i=m+1}^d u_i^T S u_i$. Pick $u_1, \dots, u_m$ to be top eigenvectors of $S$.

\subsection*{PCA Applications}

Encoder, decoder: $U = [u_1, \dots, u_m] \in \mathbb{R}^{d \times m}$. $x \rightarrow U^T x \rightarrow UU^T x$, first step is encoding, second step is decoding.\\
Principle Component Regression: eigendecomposition ($X^T X = U \Lambda U^T$, $X$ is centered data matrix), encode ($x_n \rightarrow U^T x_n = \tilde{x}_n$), Ordinary Least Sq. ($\hat{\beta}_m = (\tilde{X}^T\tilde{X})^{-1}\tilde{X}^Ty$), decode ($\hat{\beta} = U \hat{\beta}_m$).

\subsection*{PCA Implementation}

Data matrix: $X = [x_1 - \bar{x}, \dots, x_N - \bar{x}]^T \in \mathbb{R}^{N \times d}$.\\
Covariance matrix: $S = \frac{1}{N}X^TX$. $\frac{1}{N}X^TXu_i = \lambda_i u_i$.\\
$\frac{1}{N}XX^T(Xu_i) = \lambda_i (Xu_i)$, so $\frac{1}{N}XX^T v_i = \lambda_i v_i$ where $v_i = X u_i$. So $v_i$ is eigenvector of $Q = \frac{1}{N}XX^T$, $u_i \propto X^T v_i$.\\
$Xu_i \propto XX^T v_i \propto Qv_i \propto \lambda_i v_i$.

\subsection*{Non-linear PCA}

Centered data ($\bar{x} = 0$), non-linear $x \rightarrow \phi(x)$. $S' = \frac{1}{N}\sum_n \phi(x_n) \phi(x_n)^T$, top eigenvectors $S'v_i = \lambda_i v_i$, $i \in [m]$.\\
$\frac{1}{N}\sum_n \phi(x_n)(\phi(x_n)^T v_i) = \lambda_i v_i$, so $v_i = \sum_n a_{in}\phi(x_n)$.\\
$S' v_i = \frac{1}{N}\sum_n \phi(x_n)\phi(x_n)^T \sum_p a_{ip}\phi(x_p) = \lambda_i \sum_p a_{ip}\phi(x_p)$, $\frac{1}{N}\sum_n \phi(x_l)^T \phi(x_n) \sum_p a_{ip}\phi(x_n)^T \phi(x_p) = \lambda_i \sum_p a_{ip}\phi(x_l)^T \phi(x_p)$, $\frac{1}{N}\sum_n K(x_l, x_n) \sum_p a_{ip}K(x_n, x_p) = \lambda_i \sum_p a_{ip}K(x_l, x_p)$.\\
$K^2 a_i = \lambda_i NKa_i$, $Ka_i = \lambda_i Na_i$.\\
Projection: ($i$-th dim of projected $x$) $y_i(x) = \phi(x)^T v_i = \sum_n a_{in}\phi(x)^T\phi(x_n) = \sum_n a_{in} K(x, x_n)$.

\subsection*{Spiked Covariance Model}

Latent direction of interest: $v \in \mathbb{R}^d$, $v^Tv = ||v||^2 = 1$.\\
Data: $X_i = Y_i v + Z_i$, $Y_i \sim N(0, 1)$, $Z_i \sim N(0, \sigma^2I_{d\times d})$.\\
Mean: $E[X_i] = E[Y_i]v + E[Z_i] = 0$.\\
Covariance: $\Sigma = E[X_iX_i^T] = E[Y_i^2]vv^T + E[Z_iZ_i^T] = vv^T + \sigma^2I$.\\
Principle component (eigenvector) of covariance: $(vv^T + \sigma^2 I)v=v(v^Tv)+\sigma^2 v = (1+\sigma^2)v$.\\
Latent direction of interest captured by eigenvector of covariance matrix. PCA does that using empirical covariance.\\
Empirical Cov.: $S = \frac{1}{N}\sum_i X_i X_i^T$.\\
Use matrix concentration: with prob. $1-\delta$, $||S-\Sigma||_{\text{op}} \leq \text{const} ||\Sigma||_{\text{op}} \min(\sqrt{\frac{d+\log\frac{1}{\delta}}{N}}, \frac{d+\log\frac{1}{\delta}}{N})$. i.e., when $N \sim d$, $S \approx \Sigma$. If $k$-sparse, replace $d$ with $k$.\\
Use Davis-Kahan $\sin(\theta)$: $A$, $B$ are $d\times d$ PSD. $(\lambda_i, u_i)$, $(\mu_i, v_i)$ ranked eigenvalue/vector-pairs. $\sin(\angle(u_1, v_1))\leq \frac{2}{\max(\lambda_1 - \lambda_2, \mu_1 - \mu_2)} ||A-B||_{\text{op}}$. And $\min_{\epsilon\in{\pm1}}|\epsilon u_1 - v_1|_2^2 \leq 2 \sin^2(\angle(u_1, v_1))$.

\subsection*{Random Projection}

Data: $x_1, \dots, x_N$, $x_n \in \mathbb{R}^d$. Project to $\mathbb{R}^m$ and preserve distances.\\
Method: sample $m$ random vectors $W_1, \dots, W_m \in N(0, I_{d\times d})$, project $x_n$ as $(W_1^Tx_n, \dots, W_m^Tx_n)$.\\
Johnson-Lindenstrauss: $\epsilon, \delta \in (0, 1)$, let $m \geq \text{const}\frac{\log N/\delta}{\epsilon^2}$. Then for $1 \leq i, j \leq N$, $1-\epsilon \leq \frac{\frac{1}{\sqrt{m}}||W x_i - W x_j||_2}{||x_i - x_j||_2} \leq 1+\epsilon$ with prob. $1-\delta$. (i.e. projecting into $\log N$-dim preserves distance.)\\
Proof: Let $x = x_i - x_j$ for $||x||_2=1$ then we interested in $\frac{1}{\sqrt{m}}||Wx||_2$. Since $Wx \sim N(0, I_{m\times m})$, $||Wx||_2^2 = \sum_{i=1}^m Z_i^2$ where $Z_i \sim N(0, 1)$. Use Chernoff, $P(|\frac{1}{m}\sum_{i=1}^m Z_i^2-1| \geq \epsilon) \leq 2 \exp(-m\epsilon^2/6)$. Select $m \geq \text{const}\frac{\log N / \delta}{\epsilon^2}$.


